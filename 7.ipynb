{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc9ebcdd",
   "metadata": {},
   "source": [
    "AdaBoost, short for Adaptive Boosting, is a machine learning algorithm designed for binary classification tasks. It works by combining multiple weak learners (typically decision trees with a single split, often called stumps) to create a strong ensemble model. AdaBoost is an iterative algorithm that adjusts the weights of incorrectly classified instances in the training set to focus more on the difficult-to-classify instances in subsequent iterations.\n",
    "\n",
    "Here's how AdaBoost works:\n",
    "\n",
    "Initialization:\n",
    "\n",
    "Each instance in the training dataset is assigned an equal weight initially. These weights represent the importance of each instance in the training process.\n",
    "Building Weak Learners:\n",
    "\n",
    "AdaBoost starts by training a weak learner (often a decision stump) on the training dataset. The weak learner tries to classify the instances based on the features provided.\n",
    "At the beginning, all features are given equal importance. The weak learner aims to find the feature and threshold that minimizes the classification error (or maximizes accuracy) on the weighted training data.\n",
    "Weighted Error Calculation:\n",
    "\n",
    "After the weak learner is trained, its performance on the training data is evaluated. The error of the weak learner is calculated as the weighted sum of misclassified instances, where the weights are the importance weights assigned to each instance.\n",
    "Instances that are misclassified by the weak learner are given higher importance weights for the next iteration, while correctly classified instances are given lower weights. This way, AdaBoost focuses more on the instances that are difficult to classify.\n",
    "Updating Instance Weights:\n",
    "\n",
    "The weights of the instances are updated based on their classification performance. Misclassified instances have their weights increased, while correctly classified instances have their weights decreased. This way, the subsequent weak learners focus more on the instances that were misclassified by the previous learners.\n",
    "Weighted Model Aggregation:\n",
    "\n",
    "AdaBoost combines the predictions of all the weak learners into a final strong learner (ensemble model). Each weak learner's contribution to the final prediction is weighted based on its accuracy, with more accurate learners having higher weights.\n",
    "Final Prediction:\n",
    "\n",
    "To make a prediction on a new instance, AdaBoost aggregates the predictions of all the weak learners, where each learner's prediction is weighted by its importance in the ensemble.\n",
    "Typically, AdaBoost uses a weighted majority vote to determine the final predicted class label.\n",
    "AdaBoost continues the iterative process of training weak learners and updating instance weights until a predefined number of iterations is reached or until the model achieves satisfactory performance on the training data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
